{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python version: 3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]\n",
      "CNTK version: 2.4\n",
      "NumPy version: 1.19.2\n",
      "Pandas version: 1.1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    import sys,cntk\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    print('''\n",
    "    Python version: {}\n",
    "    CNTK version: {}\n",
    "    NumPy version: {}\n",
    "    Pandas version: {}\n",
    "    '''.format(sys.version, cntk.__version__, np.__version__, pd.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "input_files = ['C:/Users/PRABU/Trexquant/words_250000_train.txt',\n",
    "               ]\n",
    "\n",
    "for filename in input_files:\n",
    "    with open(filename, 'r') as f:\n",
    "        # skip the header lines\n",
    "        for i in range(1):\n",
    "            f.readline()\n",
    "\n",
    "        for line in f:\n",
    "            word = line.split(' ')[0]\n",
    "            if word.isalpha():\n",
    "                word_dict[word.lower()] = None\n",
    "\n",
    "word_dict['microsoft'] = None\n",
    "word_dict['cntk'] = None\n",
    "\n",
    "# create a list to be used as input later\n",
    "words = list(np.random.permutation(list(word_dict.keys())))\n",
    "with open('word_list.txt', 'w') as f:\n",
    "    for word in words:\n",
    "        f.write('{}\\n'.format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"split\"></a>\n",
    "## Partition the words into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1 WordNet words\n",
      "Max word length: 9, average word length: 9.0\n"
     ]
    }
   ],
   "source": [
    "# During training, the model will only see words below this index.\n",
    "# The remainder of the words can be used as a validation set.\n",
    "train_val_split_idx = int(len(list(word_dict.keys())) * 0.8)\n",
    "print('Training with {} WordNet words'.format(train_val_split_idx))\n",
    "\n",
    "MAX_NUM_INPUTS = max([len(i) for i in words[:train_val_split_idx]])\n",
    "EPOCH_SIZE = train_val_split_idx\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = np.array([len(i) for i in words[:train_val_split_idx]]).mean()\n",
    "print('Max word length: {}, average word length: {:0.1f}'.format(MAX_NUM_INPUTS, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"player\"></a>\n",
    "## Create the game player\n",
    "\n",
    "This is a \"wrapper\" of sorts around our neural network model, that handles the dynamics of gameplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanPlayer:\n",
    "    def __init__(self, word, model, lives=10):\n",
    "        self.original_word = word\n",
    "        self.full_word = [ord(i)-97 for i in word]\n",
    "        self.letters_guessed = set([])\n",
    "        self.letters_remaining = set(self.full_word)\n",
    "        self.lives_left = lives\n",
    "        self.obscured_words_seen = []\n",
    "        self.letters_previously_guessed = []\n",
    "        self.guesses = []\n",
    "        self.correct_responses = []\n",
    "        self.z = model\n",
    "        return\n",
    "    \n",
    "    def encode_obscured_word(self):\n",
    "        word = [i if i in self.letters_guessed else 26 for i in self.full_word]\n",
    "        obscured_word = np.zeros((len(word), 27), dtype=np.float32)\n",
    "        for i, j in enumerate(word):\n",
    "            obscured_word[i, j] = 1\n",
    "        return(obscured_word)\n",
    "    \n",
    "    def encode_guess(self, guess):\n",
    "        encoded_guess = np.zeros(26, dtype=np.float32)\n",
    "        encoded_guess[guess] = 1\n",
    "        return(encoded_guess)\n",
    "\n",
    "    def encode_previous_guesses(self):\n",
    "        # Create a 1 x 26 vector where 1s indicate that the letter was previously guessed\n",
    "        guess = np.zeros(26, dtype=np.float32)\n",
    "        for i in self.letters_guessed:\n",
    "            guess[i] = 1\n",
    "        return(guess)\n",
    "    \n",
    "    def encode_correct_responses(self):\n",
    "        # To be used with cross_entropy_with_softmax, this vector must be normalized\n",
    "        response = np.zeros(26, dtype=np.float32)\n",
    "        for i in self.letters_remaining:\n",
    "            response[i] = 1.0\n",
    "        response /= response.sum()\n",
    "        return(response)\n",
    "    \n",
    "    def store_guess_and_result(self, guess):\n",
    "        # Record what the model saw as input: an obscured word and a list of previously-guessed letters\n",
    "        self.obscured_words_seen.append(self.encode_obscured_word())\n",
    "        self.letters_previously_guessed.append(self.encode_previous_guesses())\n",
    "        \n",
    "        # Record the letter that the model guessed, and add that guess to the list of previous guesses\n",
    "        self.guesses.append(guess)\n",
    "        self.letters_guessed.add(guess)\n",
    "        \n",
    "        # Store the \"correct responses\"\n",
    "        correct_responses = self.encode_correct_responses()\n",
    "        self.correct_responses.append(correct_responses)\n",
    "        \n",
    "        # Determine an appropriate reward, and reduce # of lives left if appropriate\n",
    "        if guess in self.letters_remaining:\n",
    "            self.letters_remaining.remove(guess)\n",
    "        \n",
    "        if self.correct_responses[-1][guess] < 0.00001:\n",
    "            self.lives_left -= 1\n",
    "        return\n",
    "                \n",
    "    def run(self):\n",
    "        # Play a game until we run out of lives or letters\n",
    "        while (self.lives_left > 0) and (len(self.letters_remaining) > 0):\n",
    "            guess = np.argmax(np.squeeze(self.z.eval({self.z.arguments[0]: np.array(self.encode_obscured_word()),\n",
    "                                                      self.z.arguments[1]: np.array(self.encode_previous_guesses())})))\n",
    "            self.store_guess_and_result(guess)\n",
    "        \n",
    "        # Return the observations for use in training (both inputs, predictions, and losses)\n",
    "        return(np.array(self.obscured_words_seen),\n",
    "               np.array(self.letters_previously_guessed),\n",
    "               np.array(self.correct_responses))\n",
    "    \n",
    "    def show_words_seen(self):\n",
    "        for word in self.obscured_words_seen:\n",
    "            print(''.join([chr(i + 97) if i != 26 else ' ' for i in word.argmax(axis=1)]))\n",
    "            \n",
    "    def show_guesses(self):\n",
    "        for guess in self.guesses:\n",
    "            print(chr(guess + 97))\n",
    "            \n",
    "    def play_by_play(self):\n",
    "        print('Hidden word was \"{}\"'.format(self.original_word))\n",
    "        for i in range(len(self.guesses)):\n",
    "            word_seen = ''.join([chr(i + 97) if i != 26 else ' ' for i in self.obscured_words_seen[i].argmax(axis=1)])\n",
    "            print('Guessed {} after seeing \"{}\"'.format(chr(self.guesses[i] + 97),\n",
    "                                                        word_seen))\n",
    "            \n",
    "    def evaluate_performance(self):\n",
    "        # Assumes that the run() method has already been called\n",
    "        ended_in_success = self.lives_left > 0\n",
    "        letters_in_word = set([i for i in self.original_word])\n",
    "        correct_guesses = len(letters_in_word) - len(self.letters_remaining)\n",
    "        incorrect_guesses = len(self.guesses) - correct_guesses\n",
    "        return(ended_in_success, correct_guesses, incorrect_guesses, letters_in_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model\"></a>\n",
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_net(input_obscured_word_seen, input_letters_guessed_previously):\n",
    "    with cntk.layers.default_options(initial_state = 0.1):\n",
    "        lstm_outputs = cntk.layers.Recurrence(cntk.layers.LSTM(MAX_NUM_INPUTS))(input_obscured_word_seen)\n",
    "        final_lstm_output = cntk.ops.sequence.last(lstm_outputs)\n",
    "        combined_input = cntk.ops.splice(final_lstm_output, input_letters_guessed_previously)\n",
    "        dense_layer = cntk.layers.Dense(26, name='final_dense_layer')(combined_input)\n",
    "        return(dense_layer)\n",
    "    \n",
    "input_obscured_word_seen = cntk.ops.input_variable(shape=27,\n",
    "                                                   dynamic_axes=[cntk.Axis.default_batch_axis(),\n",
    "                                                                 cntk.Axis.default_dynamic_axis()],\n",
    "                                                   name='input_obscured_word_seen')\n",
    "input_letters_guessed_previously = cntk.ops.input_variable(shape=26,\n",
    "                                                           dynamic_axes=[cntk.Axis.default_batch_axis()],\n",
    "                                                           name='input_letters_guessed_previously')\n",
    "\n",
    "z = create_LSTM_net(input_obscured_word_seen, input_letters_guessed_previously)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train\"></a>\n",
    "## Train the model\n",
    "\n",
    "Set some learning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and displayed metric\n",
    "input_correct_responses = cntk.ops.input_variable(shape=26,\n",
    "                                                  dynamic_axes=[cntk.Axis.default_batch_axis()],\n",
    "                                                  name='input_correct_responses')\n",
    "pe = cntk.losses.cross_entropy_with_softmax(z, input_correct_responses)\n",
    "ce = cntk.metrics.classification_error(z, input_correct_responses)\n",
    "\n",
    "learning_rate = 0.1\n",
    "lr_schedule = cntk.learners.learning_rate_schedule(learning_rate, cntk.UnitType.minibatch)\n",
    "momentum_time_constant = cntk.learners.momentum_as_time_constant_schedule(BATCH_SIZE / -np.log(0.9)) \n",
    "learner = cntk.learners.fsadagrad(z.parameters,\n",
    "                                  lr=lr_schedule,\n",
    "                                  momentum=momentum_time_constant,\n",
    "                                  unit_gain = True)\n",
    "trainer = cntk.Trainer(z, (pe, ce), learner)\n",
    "progress_printer = cntk.logging.progress_print.ProgressPrinter(freq=EPOCH_SIZE, tag='Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the actual training using the code cell below. Note that this step will take many hours to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minibatch[   1-   1]: loss = 3.349436 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[1]: [Training] loss = 3.349436 * 12, metric = 97.77% * 12 0.433s ( 27.7 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.324976 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[2]: [Training] loss = 3.324976 * 12, metric = 97.77% * 12 0.249s ( 48.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.297099 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[3]: [Training] loss = 3.297099 * 12, metric = 97.77% * 12 0.255s ( 47.1 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.260146 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[4]: [Training] loss = 3.260146 * 12, metric = 97.77% * 12 0.278s ( 43.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.319223 * 13, metric = 96.66% * 13;\n",
      "Finished Epoch[5]: [Training] loss = 3.319223 * 13, metric = 96.66% * 13 0.292s ( 44.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.245197 * 13, metric = 96.66% * 13;\n",
      "Finished Epoch[6]: [Training] loss = 3.245197 * 13, metric = 96.66% * 13 0.265s ( 49.1 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.185523 * 13, metric = 96.66% * 13;\n",
      "Finished Epoch[7]: [Training] loss = 3.185523 * 13, metric = 96.66% * 13 0.285s ( 45.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.163990 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[8]: [Training] loss = 3.163990 * 12, metric = 97.77% * 12 0.250s ( 48.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.146003 * 13, metric = 96.66% * 13;\n",
      "Finished Epoch[9]: [Training] loss = 3.146003 * 13, metric = 96.66% * 13 0.259s ( 50.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.064451 * 11, metric = 98.86% * 11;\n",
      "Finished Epoch[10]: [Training] loss = 3.064451 * 11, metric = 98.86% * 11 0.223s ( 49.3 samples/s);\n",
      " Minibatch[   1-   1]: loss = 3.024691 * 11, metric = 98.86% * 11;\n",
      "Finished Epoch[11]: [Training] loss = 3.024691 * 11, metric = 98.86% * 11 0.232s ( 47.4 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.962007 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[12]: [Training] loss = 2.962007 * 12, metric = 97.77% * 12 0.274s ( 43.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.901683 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[13]: [Training] loss = 2.901683 * 12, metric = 97.77% * 12 0.246s ( 48.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.831639 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[14]: [Training] loss = 2.831639 * 12, metric = 97.77% * 12 0.241s ( 49.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.820013 * 13, metric = 96.66% * 13;\n",
      "Finished Epoch[15]: [Training] loss = 2.820013 * 13, metric = 96.66% * 13 0.286s ( 45.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.754848 * 13, metric = 96.66% * 13;\n",
      "Finished Epoch[16]: [Training] loss = 2.754848 * 13, metric = 96.66% * 13 0.261s ( 49.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.721887 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[17]: [Training] loss = 2.721887 * 12, metric = 97.77% * 12 0.233s ( 51.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.679473 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[18]: [Training] loss = 2.679473 * 12, metric = 97.77% * 12 0.251s ( 47.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.459704 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[19]: [Training] loss = 2.459704 * 15, metric = 94.10% * 15 0.308s ( 48.7 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.288747 * 16, metric = 92.39% * 16;\n",
      "Finished Epoch[20]: [Training] loss = 2.288747 * 16, metric = 92.39% * 16 0.302s ( 53.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.428369 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[21]: [Training] loss = 2.428369 * 15, metric = 94.10% * 15 0.270s ( 55.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.586841 * 12, metric = 97.77% * 12;\n",
      "Finished Epoch[22]: [Training] loss = 2.586841 * 12, metric = 97.77% * 12 0.218s ( 55.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.462620 * 14, metric = 95.47% * 14;\n",
      "Finished Epoch[23]: [Training] loss = 2.462620 * 14, metric = 95.47% * 14 0.251s ( 55.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.401667 * 14, metric = 95.47% * 14;\n",
      "Finished Epoch[24]: [Training] loss = 2.401667 * 14, metric = 95.47% * 14 0.261s ( 53.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.374998 * 13, metric = 96.66% * 13;\n",
      "Finished Epoch[25]: [Training] loss = 2.374998 * 13, metric = 96.66% * 13 0.261s ( 49.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.333439 * 14, metric = 95.47% * 14;\n",
      "Finished Epoch[26]: [Training] loss = 2.333439 * 14, metric = 95.47% * 14 0.270s ( 51.9 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.245014 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[27]: [Training] loss = 2.245014 * 15, metric = 94.10% * 15 0.303s ( 49.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.141514 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[28]: [Training] loss = 2.141514 * 15, metric = 94.10% * 15 0.283s ( 53.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.058355 * 16, metric = 92.39% * 16;\n",
      "Finished Epoch[29]: [Training] loss = 2.058355 * 16, metric = 92.39% * 16 0.311s ( 51.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.952191 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[30]: [Training] loss = 1.952191 * 17, metric = 89.89% * 17 0.298s ( 57.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.158811 * 14, metric = 95.47% * 14;\n",
      "Finished Epoch[31]: [Training] loss = 2.158811 * 14, metric = 95.47% * 14 0.268s ( 52.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.145140 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[32]: [Training] loss = 2.145140 * 15, metric = 94.10% * 15 0.272s ( 55.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.078039 * 16, metric = 92.39% * 16;\n",
      "Finished Epoch[33]: [Training] loss = 2.078039 * 16, metric = 92.39% * 16 0.287s ( 55.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.995460 * 16, metric = 92.39% * 16;\n",
      "Finished Epoch[34]: [Training] loss = 1.995460 * 16, metric = 92.39% * 16 0.303s ( 52.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.105827 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[35]: [Training] loss = 2.105827 * 15, metric = 94.10% * 15 0.266s ( 56.4 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.058986 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[36]: [Training] loss = 2.058986 * 15, metric = 94.10% * 15 0.268s ( 56.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.179832 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[37]: [Training] loss = 2.179832 * 15, metric = 94.10% * 15 0.271s ( 55.4 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.231578 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[38]: [Training] loss = 2.231578 * 15, metric = 94.10% * 15 0.270s ( 55.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.174833 * 15, metric = 94.10% * 15;\n",
      "Finished Epoch[39]: [Training] loss = 2.174833 * 15, metric = 94.10% * 15 0.265s ( 56.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 2.065760 * 16, metric = 92.39% * 16;\n",
      "Finished Epoch[40]: [Training] loss = 2.065760 * 16, metric = 92.39% * 16 0.290s ( 55.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.991379 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[41]: [Training] loss = 1.991379 * 17, metric = 89.89% * 17 0.295s ( 57.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.960878 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[42]: [Training] loss = 1.960878 * 17, metric = 89.89% * 17 0.296s ( 57.4 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.884655 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[43]: [Training] loss = 1.884655 * 17, metric = 89.89% * 17 0.300s ( 56.7 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.920066 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[44]: [Training] loss = 1.920066 * 8, metric = 66.03% * 8 0.158s ( 50.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.865914 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[45]: [Training] loss = 1.865914 * 8, metric = 66.03% * 8 0.164s ( 48.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.891931 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[46]: [Training] loss = 1.891931 * 8, metric = 66.03% * 8 0.159s ( 50.3 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.951154 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[47]: [Training] loss = 1.951154 * 8, metric = 66.03% * 8 0.162s ( 49.4 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.871083 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[48]: [Training] loss = 1.871083 * 17, metric = 89.89% * 17 0.306s ( 55.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.801239 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[49]: [Training] loss = 1.801239 * 17, metric = 89.89% * 17 0.296s ( 57.4 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.945563 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[50]: [Training] loss = 1.945563 * 17, metric = 89.89% * 17 0.297s ( 57.2 samples/s);\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minibatch[   1-   1]: loss = 1.825924 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[51]: [Training] loss = 1.825924 * 17, metric = 89.89% * 17 0.300s ( 56.7 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.860989 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[52]: [Training] loss = 1.860989 * 8, metric = 66.03% * 8 0.151s ( 53.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.859957 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[53]: [Training] loss = 1.859957 * 8, metric = 66.03% * 8 0.161s ( 49.7 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.857420 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[54]: [Training] loss = 1.857420 * 8, metric = 66.03% * 8 0.153s ( 52.3 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.887828 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[55]: [Training] loss = 1.887828 * 8, metric = 66.03% * 8 0.163s ( 49.1 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.859850 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[56]: [Training] loss = 1.859850 * 17, metric = 89.89% * 17 0.339s ( 50.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.932562 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[57]: [Training] loss = 1.932562 * 8, metric = 66.03% * 8 0.173s ( 46.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.738642 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[58]: [Training] loss = 1.738642 * 17, metric = 89.89% * 17 0.348s ( 48.9 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.882491 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[59]: [Training] loss = 1.882491 * 8, metric = 66.03% * 8 0.167s ( 47.9 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.862138 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[60]: [Training] loss = 1.862138 * 8, metric = 66.03% * 8 0.179s ( 44.7 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.854479 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[61]: [Training] loss = 1.854479 * 8, metric = 66.03% * 8 0.165s ( 48.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.835262 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[62]: [Training] loss = 1.835262 * 8, metric = 66.03% * 8 0.181s ( 44.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.830155 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[63]: [Training] loss = 1.830155 * 8, metric = 66.03% * 8 0.166s ( 48.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.821988 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[64]: [Training] loss = 1.821988 * 8, metric = 66.03% * 8 0.188s ( 42.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.795550 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[65]: [Training] loss = 1.795550 * 8, metric = 66.03% * 8 0.167s ( 47.9 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.779825 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[66]: [Training] loss = 1.779825 * 8, metric = 66.03% * 8 0.180s ( 44.4 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.766946 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[67]: [Training] loss = 1.766946 * 8, metric = 66.03% * 8 0.170s ( 47.1 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.680802 * 17, metric = 89.89% * 17;\n",
      "Finished Epoch[68]: [Training] loss = 1.680802 * 17, metric = 89.89% * 17 0.320s ( 53.1 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.722196 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[69]: [Training] loss = 1.722196 * 8, metric = 66.03% * 8 0.169s ( 47.3 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.680463 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[70]: [Training] loss = 1.680463 * 8, metric = 66.03% * 8 0.179s ( 44.7 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.696196 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[71]: [Training] loss = 1.696196 * 8, metric = 66.03% * 8 0.164s ( 48.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.658217 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[72]: [Training] loss = 1.658217 * 8, metric = 66.03% * 8 0.181s ( 44.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.653353 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[73]: [Training] loss = 1.653353 * 8, metric = 66.03% * 8 0.173s ( 46.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.634159 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[74]: [Training] loss = 1.634159 * 8, metric = 66.03% * 8 0.182s ( 44.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.625721 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[75]: [Training] loss = 1.625721 * 8, metric = 66.03% * 8 0.166s ( 48.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.635566 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[76]: [Training] loss = 1.635566 * 8, metric = 66.03% * 8 0.185s ( 43.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.628247 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[77]: [Training] loss = 1.628247 * 8, metric = 66.03% * 8 0.165s ( 48.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.615928 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[78]: [Training] loss = 1.615928 * 8, metric = 66.03% * 8 0.180s ( 44.4 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.627744 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[79]: [Training] loss = 1.627744 * 8, metric = 66.03% * 8 0.177s ( 45.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.646626 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[80]: [Training] loss = 1.646626 * 8, metric = 66.03% * 8 0.277s ( 28.9 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.639502 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[81]: [Training] loss = 1.639502 * 8, metric = 66.03% * 8 0.166s ( 48.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.633865 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[82]: [Training] loss = 1.633865 * 8, metric = 66.03% * 8 0.183s ( 43.7 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.622485 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[83]: [Training] loss = 1.622485 * 8, metric = 66.03% * 8 0.190s ( 42.1 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.625478 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[84]: [Training] loss = 1.625478 * 8, metric = 66.03% * 8 0.182s ( 44.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.618650 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[85]: [Training] loss = 1.618650 * 8, metric = 66.03% * 8 0.168s ( 47.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.611156 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[86]: [Training] loss = 1.611156 * 8, metric = 66.03% * 8 0.180s ( 44.4 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.603752 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[87]: [Training] loss = 1.603752 * 8, metric = 66.03% * 8 0.167s ( 47.9 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.594585 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[88]: [Training] loss = 1.594585 * 8, metric = 66.03% * 8 0.185s ( 43.2 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.584006 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[89]: [Training] loss = 1.584006 * 8, metric = 66.03% * 8 0.171s ( 46.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.572394 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[90]: [Training] loss = 1.572394 * 8, metric = 66.03% * 8 0.186s ( 43.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.560130 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[91]: [Training] loss = 1.560130 * 8, metric = 66.03% * 8 0.172s ( 46.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.555237 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[92]: [Training] loss = 1.555237 * 8, metric = 66.03% * 8 0.164s ( 48.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.543881 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[93]: [Training] loss = 1.543881 * 8, metric = 66.03% * 8 0.156s ( 51.3 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.529269 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[94]: [Training] loss = 1.529269 * 8, metric = 66.03% * 8 0.165s ( 48.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.540571 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[95]: [Training] loss = 1.540571 * 8, metric = 66.03% * 8 0.154s ( 52.0 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.566955 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[96]: [Training] loss = 1.566955 * 8, metric = 66.03% * 8 0.172s ( 46.5 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.566402 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[97]: [Training] loss = 1.566402 * 8, metric = 66.03% * 8 0.159s ( 50.3 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.549712 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[98]: [Training] loss = 1.549712 * 8, metric = 66.03% * 8 0.164s ( 48.8 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.546932 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[99]: [Training] loss = 1.546932 * 8, metric = 66.03% * 8 0.158s ( 50.6 samples/s);\n",
      " Minibatch[   1-   1]: loss = 1.554182 * 8, metric = 66.03% * 8;\n",
      "Finished Epoch[100]: [Training] loss = 1.554182 * 8, metric = 66.03% * 8 0.167s ( 47.9 samples/s);\n"
     ]
    }
   ],
   "source": [
    "total_samples = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    i = 0\n",
    "    while total_samples < (epoch+1) * EPOCH_SIZE:\n",
    "        word = words[i]\n",
    "        i += 1\n",
    "        \n",
    "        other_player = HangmanPlayer(word, z)\n",
    "        words_seen, previous_letters, correct_responses = other_player.run()\n",
    "        \n",
    "        trainer.train_minibatch({input_obscured_word_seen: words_seen,\n",
    "                                 input_letters_guessed_previously: previous_letters,\n",
    "                                 input_correct_responses: correct_responses})\n",
    "        total_samples += 1\n",
    "        progress_printer.update_with_trainer(trainer, with_metric=True)\n",
    "        \n",
    "    progress_printer.epoch_summary(with_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = './hangman_model.dnn'\n",
    "z.save(model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"eval\"></a>\n",
    "## Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden word was \"microsoft\"\n",
      "Guessed t after seeing \"         \"\n",
      "Guessed o after seeing \"        t\"\n",
      "Guessed r after seeing \"    o o t\"\n",
      "Guessed s after seeing \"   ro o t\"\n",
      "Guessed i after seeing \"   roso t\"\n",
      "Guessed m after seeing \" i roso t\"\n",
      "Guessed c after seeing \"mi roso t\"\n",
      "Guessed f after seeing \"microso t\"\n"
     ]
    }
   ],
   "source": [
    "other_player.play_by_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific case, the performance is encouraging!\n",
    "\n",
    "### More thorough version of performance evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(my_words, my_model):\n",
    "    results = []\n",
    "    for word in my_words:\n",
    "        my_player = HangmanPlayer(word, my_model)\n",
    "        _ = my_player.run()\n",
    "        results.append(my_player.evaluate_performance())\n",
    "    df = pd.DataFrame(results, columns=['won', 'num_correct', 'num_incorrect', 'letters'])\n",
    "    return(df)\n",
    "\n",
    "# Expect this to take roughly ten minutes\n",
    "result_df = evaluate_model(words[train_val_split_idx:], z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we summarize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the validation set:\n",
      "- Averaged 2.0 correct and 10.0 incorrect guesses per game\n",
      "- Won 0.0% of games played\n"
     ]
    }
   ],
   "source": [
    "print('Performance on the validation set:')\n",
    "print('- Averaged {:0.1f} correct and {:0.1f} incorrect guesses per game'.format(result_df['num_correct'].mean(),\n",
    "                                                                       result_df['num_incorrect'].mean()))\n",
    "print('- Won {:0.1f}% of games played'.format(100 * result_df['won'].sum() / len(result_df.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fun\"></a>\n",
    "## Just for fun -- play hangman with your favorite word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden word was \"microsoft\"\n",
      "Guessed t after seeing \"         \"\n",
      "Guessed o after seeing \"        t\"\n",
      "Guessed r after seeing \"    o o t\"\n",
      "Guessed s after seeing \"   ro o t\"\n",
      "Guessed i after seeing \"   roso t\"\n",
      "Guessed m after seeing \" i roso t\"\n",
      "Guessed c after seeing \"mi roso t\"\n",
      "Guessed f after seeing \"microso t\"\n"
     ]
    }
   ],
   "source": [
    "model_filename = './hangman_model.dnn'\n",
    "z2 = cntk.load_model(model_filename)\n",
    "my_word = 'microsoft'\n",
    "\n",
    "my_player = HangmanPlayer(my_word, z2)\n",
    "_ = my_player.run()\n",
    "my_player.play_by_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model won this game\n",
      "The model made 8 correct guesses and 0 incorrect guesses\n"
     ]
    }
   ],
   "source": [
    "results = my_player.evaluate_performance()\n",
    "print('The model {} this game'.format('won' if results[0] else 'did not win'))\n",
    "print('The model made {} correct guesses and {} incorrect guesses'.format(results[1], results[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
